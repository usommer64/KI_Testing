#%%
"""
Vectorstore f√ºr IBM Lizenzierungsdokumente
Verwendet ChromaDB + BGE-Large-en-v1.5 Embeddings
Neu mit adaptiver Cchunk Verteilung
Adaptive Chunk-Size basierend auf Wort-Count.
"""

import pandas as pd #neu eingef√ºgt
from pathlib import Path
from typing import List, Optional
import logging

from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from langchain.schema import Document

from loader import LicenseDocumentLoader

# Logging konfigurieren
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

#neu eingef√ºgt Anfang

STATS_CSV = Path(__file__).parent.parent / "data" / "document_stats.csv"

def get_chunk_size_by_words(word_count):
    """Adaptive Chunk-Size."""
    if word_count < 1000:
        return 500, 125
    elif word_count < 2000:
        return 450, 110
    elif word_count < 3500:
        return 400, 100
    elif word_count < 5000:
        return 350, 90
    elif word_count < 7000:
        return 300, 75
    else:
        return 250, 60

#neu eingef√ºgt Ende

class LicenseVectorStore:
    """
    Vektordatenbank f√ºr Lizenzdokumente.
    
    Features:
    - BGE-Large-en-v1.5 Embeddings (Top-Qualit√§t)
    - ChromaDB (persistent, lokal)
    - Asymmetrische Suche (Query-Prefix)
    """
    
    def __init__(
        self,
        collection_name: str = "ibm_licenses",
        persist_directory: str = None,
        embedding_model: str = "BAAI/bge-large-en-v1.5"
    ):
        """
        Args:
            collection_name: Name der ChromaDB Collection
            persist_directory: Pfad f√ºr persistente Speicherung
            embedding_model: Hugging Face Model-Name
        """
        self.collection_name = collection_name
        
        # Default: Speichern neben src/
        if persist_directory is None:
            persist_directory = str(Path(__file__).parent.parent / "data" / "chroma_db")
        
        self.persist_directory = persist_directory
        
        # Embedding-Modell laden
        logger.info(f"üì• Lade Embedding-Modell: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        logger.info(f"‚úÖ Modell geladen: {self.embedding_model.get_sentence_embedding_dimension()} Dimensionen")
        
        # ChromaDB Client erstellen
        logger.info(f"üìÇ Initialisiere ChromaDB in: {persist_directory}")
        self.client = chromadb.PersistentClient(
            path=str(self.persist_directory),
            settings=Settings(
                anonymized_telemetry=False # Keine Telemetrie
            )
        )  
        
        
        # Collection erstellen oder laden
        try:
            self.collection = self.client.get_collection(name=collection_name)
            logger.info(f"‚úÖ Collection '{collection_name}' geladen ({self.collection.count()} Dokumente)")
        except Exception:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "IBM Licensing Documents"}
            )
            logger.info(f"‚úÖ Collection '{collection_name}' erstellt")
    
    def embed_texts(self, texts: List[str], is_query: bool = False) -> List[List[float]]:
        """
        Erstellt Embeddings f√ºr Texte.
        
        Args:
            texts: Liste von Texten
            is_query: True f√ºr Queries (nutzt Query-Prefix)
            
        Returns:
            Liste von Embedding-Vektoren
        """
        if is_query:
            # Query-Prefix f√ºr bessere Retrieval-Qualit√§t
            texts = [
                f"Represent this sentence for searching relevant passages: {text}"
                for text in texts
            ]
        
        # Embeddings erstellen
        embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        return embeddings.tolist()
    
    def add_documents(self, documents: List[Document]) -> None:
        """
        F√ºgt Dokumente zur Vektordatenbank hinzu.
        
        Args:
            documents: Liste von LangChain Document-Objekten
        """
        if not documents:
            logger.warning("‚ö†Ô∏è  Keine Dokumente zum Hinzuf√ºgen!")
            return
        
        logger.info(f"üìù Bereite {len(documents)} Dokumente vor...")
        
        # Texte extrahieren
        texts = [doc.page_content for doc in documents]
        
        # Metadaten vorbereiten (ChromaDB akzeptiert nur bestimmte Typen)
        metadatas = []
        for doc in documents:
            metadata = {
                "source": str(doc.metadata.get("source", "unknown")),
                "file_name": doc.metadata.get("file_name", "unknown"),
                "file_type": doc.metadata.get("file_type", "unknown"),
            }
            # Page nur hinzuf√ºgen wenn vorhanden
            if "page" in doc.metadata:
                metadata["page"] = int(doc.metadata["page"])
            
            metadatas.append(metadata)
        
        # IDs generieren (eindeutige IDs f√ºr jedes Dokument)
        ids = [f"doc_{i}" for i in range(len(documents))]
        
        # Embeddings erstellen
        logger.info(f"üî¢ Erstelle Embeddings (das dauert ~30 Sekunden)...")
        embeddings = self.embed_texts(texts, is_query=False)
        
        # Zu ChromaDB hinzuf√ºgen
        logger.info(f"üíæ Speichere in ChromaDB...")
        self.collection.add(
            documents=texts,
            metadatas=metadatas,
            embeddings=embeddings,
            ids=ids
        )
        
        logger.info(f"‚úÖ {len(documents)} Dokumente hinzugef√ºgt!")
        logger.info(f"üìä Collection enth√§lt jetzt {self.collection.count()} Dokumente")
    
    def search(
        self,
        query: str,
        k: int = 5,
        filter_metadata: Optional[dict] = None
    ) -> List[dict]:
        """
        Sucht √§hnliche Dokumente.
        
        Args:
            query: Suchanfrage
            k: Anzahl Ergebnisse
            filter_metadata: Optional: Filter f√ºr Metadaten
                Beispiel: {"file_type": "pdf"}
            
        Returns:
            Liste von Ergebnissen mit Text, Metadaten, Score
        """
        logger.info(f"üîç Suche: '{query}'")
        
        # Query-Embedding erstellen (mit Prefix!)
        query_embedding = self.embed_texts([query], is_query=True)[0]
        
        # ChromaDB-Suche
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k,
            where=filter_metadata  # Optional: Filter
        )
        
        # Ergebnisse formatieren
        formatted_results = []
        for i in range(len(results['ids'][0])):
            result = {
                "id": results['ids'][0][i],
                "text": results['documents'][0][i],
                "metadata": results['metadatas'][0][i],
                "score": results['distances'][0][i]  # Niedriger = √§hnlicher
            }
            formatted_results.append(result)
        
        logger.info(f"‚úÖ {len(formatted_results)} Ergebnisse gefunden")
        return formatted_results
    
    def reset(self) -> None:
        """L√∂scht alle Dokumente aus der Collection."""
        try:
            self.client.delete_collection(name=self.collection_name)
            logger.info(f"üóëÔ∏è  Collection '{self.collection_name}' gel√∂scht")
            
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "IBM Licensing Documents"}
            )
            logger.info(f"‚úÖ Collection '{self.collection_name}' neu erstellt")
        except Exception as e:
            logger.error(f"‚ùå Fehler beim Reset: {e}")
    
    def get_stats(self) -> dict:
        """Gibt Statistiken √ºber die Datenbank zur√ºck."""
        count = self.collection.count()
        
        # Sample-Dokument f√ºr Metadaten-Info
        sample = None
        if count > 0:
            sample = self.collection.get(limit=1)
        
        return {
            "collection_name": self.collection_name,
            "total_documents": count,
            "persist_directory": self.persist_directory,
            "embedding_model": self.embedding_model,
            "embedding_dimensions": self.embedding_model.get_sentence_embedding_dimension(),
            "sample_metadata": sample['metadatas'][0] if sample else None
        }


def main():
    """Test-Funktion: L√§dt Dokumente und erstellt Vektordatenbank."""
    from pathlib import Path
    
    print("=" * 70)
    print("üöÄ VECTORSTORE SETUP")
    print("=" * 70)
    
    # Pfad zu Dokumenten
    data_dir = Path(__file__).parent.parent / "data"
    
    # 1. Dokumente laden
    print("\nüìö SCHRITT 1: Dokumente laden")
    print("-" * 70)
    loader = LicenseDocumentLoader(chunk_size=500, chunk_overlap=100)
    chunks = loader.load_and_split(data_dir)
    
    if not chunks:
        print("‚ùå Keine Dokumente gefunden!")
        return
    
    print(f"‚úÖ {len(chunks)} Chunks geladen")
    
    # 2. Vectorstore erstellen
    print("\nüî¢ SCHRITT 2: Vectorstore erstellen")
    print("-" * 70)
    vectorstore = LicenseVectorStore(
        collection_name="ibm_licenses",
        embedding_model="BAAI/bge-large-en-v1.5"
    )
    
    # 3. Dokumente hinzuf√ºgen
    print("\nüíæ SCHRITT 3: Dokumente embedden und speichern")
    print("-" * 70)
    vectorstore.add_documents(chunks)
    
    # 4. Statistiken
    print("\nüìä SCHRITT 4: Statistiken")
    print("-" * 70)
    stats = vectorstore.get_stats()
    print(f"Collection: {stats['collection_name']}")
    print(f"Dokumente: {stats['total_documents']}")
    print(f"Dimensionen: {stats['embedding_dimensions']}")
    print(f"Speicherort: {stats['persist_directory']}")
    
    # 5. Test-Queries
    print("\nüîç SCHRITT 5: Test-Queries")
    print("=" * 70)
    
    test_queries = [
        "Was ist IBM BYOSL?",
        "Wie funktioniert Container-Lizenzierung?",
        "Was bedeutet PVU?",
        "Virtualisierung und Sub-Capacity",
    ]
    
    for query in test_queries:
        print(f"\nüìù Query: '{query}'")
        print("-" * 70)
        
        results = vectorstore.search(query, k=3)
        
        for i, result in enumerate(results, 1):
            print(f"\n{i}. Score: {result['score']:.4f}")
            print(f"   Quelle: {result['metadata']['file_name']}")
            if 'page' in result['metadata']:
                print(f"   Seite: {result['metadata']['page']}")
            print(f"   Text: {result['text'][:150]}...")
    
    print("\n" + "=" * 70)
    print("‚úÖ VECTORSTORE SETUP ABGESCHLOSSEN!")
    print("=" * 70)


if __name__ == "__main__":
    main()
# %%
