#%%
"""
Vectorstore f√ºr IBM Lizenzierungsdokumente
Verwendet ChromaDB + BGE-Large-en-v1.5 Embeddings
"""

from pathlib import Path
from typing import List, Optional
import logging

from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from langchain.schema import Document

from loader import LicenseDocumentLoader

# ===== NEU: PANDAS F√úR CSV =====
import pandas as pd

# Logging konfigurieren
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===== NEU: PFAD ZUR STATISTIK-CSV =====
STATS_CSV = Path(__file__).parent.parent / "data" / "document_stats.csv"


# ===== NEU: ADAPTIVE CHUNK-SIZE FUNKTION =====
def get_chunk_size_by_words(word_count):
    """
    Adaptive Chunk-Size basierend auf Wort-Count.
    Optimiert f√ºr IBM Lizenzdokumente (577-10077 W√∂rter).
    """
    if word_count < 1000:
        return 500, 125
    elif word_count < 2000:
        return 450, 110
    elif word_count < 3500:
        return 400, 100
    elif word_count < 5000:
        return 350, 90
    elif word_count < 7000:
        return 300, 75
    else:
        return 250, 60


class LicenseVectorStore:
    """
    Vektordatenbank f√ºr Lizenzdokumente.
    
    Features:
    - BGE-Large-en-v1.5 Embeddings (Top-Qualit√§t)
    - ChromaDB (persistent, lokal)
    - Asymmetrische Suche (Query-Prefix)
    - Adaptive Chunk-Size basierend auf Dokumentl√§nge  # ‚Üê NEU
    """
    
    def __init__(
        self,
        collection_name: str = "ibm_licenses",
        persist_directory: str = None,
        embedding_model: str = "BAAI/bge-large-en-v1.5"
    ):
        """
        Args:
            collection_name: Name der ChromaDB Collection
            persist_directory: Pfad f√ºr persistente Speicherung
            embedding_model: Hugging Face Model-Name
        """
        self.collection_name = collection_name
        
        # Default: Speichern neben src/
        if persist_directory is None:
            persist_directory = str(Path(__file__).parent.parent / "data" / "chroma_db")
        
        self.persist_directory = persist_directory
        
        # Embedding-Modell laden
        logger.info(f"üì• Lade Embedding-Modell: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        logger.info(f"‚úÖ Modell geladen: {self.embedding_model.get_sentence_embedding_dimension()} Dimensionen")
        
        # ===== NEU: DOKUMENT-STATISTIKEN LADEN =====
        if STATS_CSV.exists():
            self.doc_stats = pd.read_csv(STATS_CSV).set_index('filename')
            logger.info(f"‚úÖ Dokument-Statistiken geladen: {len(self.doc_stats)} Docs")
        else:
            logger.warning(f"‚ö†Ô∏è Keine Statistiken gefunden: {STATS_CSV}")
            logger.warning("   F√ºhre 'python analyze_documents.py' aus!")
            self.doc_stats = None
        
        # ChromaDB Client erstellen
        logger.info(f"üìÇ Initialisiere ChromaDB in: {persist_directory}")
        self.client = chromadb.PersistentClient(
            path=str(self.persist_directory),
            settings=Settings(
                anonymized_telemetry=False
            )
        )
        
        # Collection erstellen oder laden
        try:
            self.collection = self.client.get_collection(name=collection_name)
            logger.info(f"‚úÖ Collection '{collection_name}' geladen ({self.collection.count()} Dokumente)")
        except Exception:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "IBM Licensing Documents"}
            )
            logger.info(f"‚úÖ Collection '{collection_name}' erstellt")
    
    def embed_texts(self, texts: List[str], is_query: bool = False) -> List[List[float]]:
        """
        Erstellt Embeddings f√ºr Texte.
        
        Args:
            texts: Liste von Texten
            is_query: True f√ºr Queries (nutzt Query-Prefix)
            
        Returns:
            Liste von Embedding-Vektoren
        """
        if is_query:
            # Query-Prefix f√ºr bessere Retrieval-Qualit√§t
            texts = [
                f"Represent this sentence for searching relevant passages: {text}"
                for text in texts
            ]
        
        # Embeddings erstellen
        embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        return embeddings.tolist()
    
    def add_documents(self, documents: List[Document]) -> None:
        """
        F√ºgt Dokumente zur Vektordatenbank hinzu.
        
        Args:
            documents: Liste von LangChain Documents
        """
        if not documents:
            logger.warning("Keine Dokumente zum Hinzuf√ºgen")
            return
        
        logger.info(f"üìÑ F√ºge {len(documents)} Dokumente hinzu...")
        
        # Texte und Metadaten extrahieren
        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]
        
        # IDs generieren
        ids = [f"doc_{i}" for i in range(len(documents))]
        
        # Embeddings erstellen
        embeddings = self.embed_texts(texts, is_query=False)
        
        # Zu ChromaDB hinzuf√ºgen
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=texts,
            metadatas=metadatas
        )
        
        logger.info(f"‚úÖ {len(documents)} Dokumente hinzugef√ºgt")
    
    def search(self, query: str, k: int = 5) -> List[dict]:
        """
        Sucht √§hnliche Dokumente.
        
        Args:
            query: Suchquery
            k: Anzahl Ergebnisse
            
        Returns:
            Liste von Dictionaries mit Ergebnissen
        """
        logger.info(f"üîç Suche: '{query}'")
        
        # Query-Embedding erstellen
        query_embedding = self.embed_texts([query], is_query=True)[0]
        
        # Suche durchf√ºhren
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )
        
        # Ergebnisse formatieren
        formatted_results = []
        for i in range(len(results['ids'][0])):
            formatted_results.append({
                'id': results['ids'][0][i],
                'document': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'score': results['distances'][0][i]
            })
        
        logger.info(f"‚úÖ {len(formatted_results)} Ergebnisse gefunden")
        return formatted_results
    
    def get_stats(self) -> dict:
        """
        Gibt Statistiken √ºber die Collection zur√ºck.
        """
        count = self.collection.count()
        return {
            'collection_name': self.collection_name,
            'total_documents': count,
            'persist_directory': self.persist_directory
        }


# ===== NEU: BUILD-FUNKTION MIT ADAPTIVER CHUNK-SIZE =====
    def build_vectorstore(
    data_dir: str = None,
    chunk_size: int = None,
    chunk_overlap: int = None
    ) -> LicenseVectorStore:
        """
        Baut die Vektordatenbank NEU auf mit adaptiver Chunk-Size.
        """
        if data_dir is None:
            data_dir = str(Path(__file__).parent.parent / "data")
    
        logger.info("="*70)
        logger.info("üèóÔ∏è  BAUE VECTORSTORE MIT ADAPTIVER CHUNK-SIZE")
        logger.info("="*70)
    
        # Vectorstore initialisieren
        vs = LicenseVectorStore()
    
        # ===== ADAPTIVE CHUNK-SIZE PRO DOKUMENT =====
        pdf_files = list(Path(data_dir).glob("*.pdf"))
        logger.info(f"üìö Verarbeite {len(pdf_files)} PDFs mit adaptiver Chunk-Size...")
        
        all_chunks = []
        
        for pdf_file in pdf_files:
            filename = pdf_file.name
            
            # Chunk-Size aus Statistiken ermitteln
            if vs.doc_stats is not None and filename in vs.doc_stats.index:
                word_count = int(vs.doc_stats.loc[filename, 'words'])
                chunk_size_adaptive, overlap_adaptive = get_chunk_size_by_words(word_count)
                
                logger.info(f"üìÑ {filename}: {word_count} W√∂rter ‚Üí "
                            f"Chunk {chunk_size_adaptive}/{overlap_adaptive}")
            else:
                # Fallback
                logger.warning(f"‚ö†Ô∏è {filename} nicht in Statistiken, nutze Default 400/100")
                chunk_size_adaptive = 400
                overlap_adaptive = 100
            
            # Loader mit angepasster Chunk-Size
            loader_temp = LicenseDocumentLoader(
                chunk_size=chunk_size_adaptive,
                chunk_overlap=overlap_adaptive
            )
            
            # Nur dieses eine PDF laden
            chunks = loader_temp.load_single_pdf(pdf_file)
            
            logger.info(f"  ‚Üí {len(chunks)} Chunks erstellt")
            all_chunks.extend(chunks)
        
        logger.info(f"‚úÖ Gesamt: {len(all_chunks)} Chunks aus {len(pdf_files)} PDFs")
        
        # Zu Vectorstore hinzuf√ºgen
        vs.add_documents(all_chunks)
        
    
    # Statistiken
    stats = vs.get_stats()
    logger.info("="*70)
    logger.info("üìä FERTIG")
    logger.info("="*70)
    logger.info(f"Collection:  {stats['collection_name']}")
    logger.info(f"Dokumente:   {stats['total_documents']:,}")
    logger.info(f"Gespeichert: {stats['persist_directory']}")
    logger.info("="*70)
    
    return vs


if __name__ == "__main__":
    # Vectorstore neu bauen
    vs = build_vectorstore()
    
    # Testsuche
    results = vs.search("Was ist Sub-Capacity Lizenzierung?", k=3)
    
    print("\nüîç TEST-SUCHE:")
    for i, result in enumerate(results, 1):
        print(f"\n{i}. Score: {result['score']:.4f}")
        print(f"   Quelle: {result['metadata'].get('source', 'unknown')}")
        print(f"   Text: {result['document'][:200]}...")